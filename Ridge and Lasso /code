# This cell will:
# 1. Load the CSV from /mnt/data/BostonHousing.csv
# 2. Auto-detect the target (MEDV or similar)
# 3. Inspect and impute missing values, handle CHAS as categorical if present
# 4. Scale numeric features, OneHotEncode categorical features (if any)
# 5. Fit LinearRegression, RidgeCV, LassoCV and report metrics & coefficients
# 6. Plot coefficient comparison and MSE plots
#
# If anything in your CSV differs (different target column name), the code will try to auto-detect and report what it found.

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# helper for nicer DataFrame display in UI (available in this environment)
try:
    from caas_jupyter_tools import display_dataframe_to_user
    have_display_tool = True
except Exception:
    have_display_tool = False

FILE = "/content/BostonHousing.csv"

# -------------------------
# Load dataset
# -------------------------
df = pd.read_csv(FILE)
print("Loaded file:", FILE)
print("Shape:", df.shape)
print("\nColumns:\n", df.columns.tolist())

# Try to detect target column
candidates = ["MEDV", "medv", "MEDIAN_VALUE", "Price", "PRICE", "price", "target", "Target", "SalePrice", "MEDV."]
target_col = None
for c in candidates:
    if c in df.columns:
        target_col = c
        break

# fallback: if MEDV not found, assume last numeric column is target (common in some CSVs)
if target_col is None:
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(numeric_cols) > 0:
        target_col = numeric_cols[-1]
        print(f"\nNo common target name found â€” using last numeric column as target: '{target_col}'")
    else:
        raise ValueError("No numeric column found to treat as target - please check your CSV.")

print("\nDetected target column:", target_col)

# Quick look & missing values
print("\nFirst 5 rows:")
display(df.head())

print("\nInfo:")
display(df.info())

print("\nMissing values per column:")
missing = df.isnull().sum()
display(missing[missing > 0] if missing.any() else "No missing values")

# -------------------------
# Prepare X, y
# -------------------------
X = df.drop(columns=[target_col])
y = df[target_col].astype(float)

# If CHAS present and appears binary, treat as categorical
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

# Special-case CHAS: many Boston datasets store as numeric 0/1 but it's categorical
if "CHAS" in numeric_features:
    nunique = X["CHAS"].nunique()
    if nunique <= 4:  # treat small-unique-count numeric columns as categorical (sensible heuristic)
        numeric_features.remove("CHAS")
        categorical_features.append("CHAS")
        X["CHAS"] = X["CHAS"].astype(object)

print("\nNumeric features:", numeric_features)
print("Categorical features:", categorical_features)

# -------------------------
# Preprocessing pipelines
# -------------------------
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

transformers = []
if numeric_features:
    transformers.append(("num", numeric_transformer, numeric_features))
if categorical_features:
    transformers.append(("cat", categorical_transformer, categorical_features))

preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")

# -------------------------
# Train/test split
# -------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit preprocessor and transform (so we can obtain feature names after transformation)
preprocessor.fit(X_train)
X_train_trans = preprocessor.transform(X_train)
X_test_trans = preprocessor.transform(X_test)

# Extract feature names after preprocessing (works with sklearn >=1.0)
feature_names = []
if numeric_features:
    feature_names.extend(numeric_features)
if categorical_features:
    try:
        ohe = preprocessor.named_transformers_["cat"].named_steps["onehot"]
        cat_ohe_names = ohe.get_feature_names_out(categorical_features).tolist()
    except Exception:
        # fallback: use the original categorical column names
        cat_ohe_names = categorical_features
    feature_names.extend(list(cat_ohe_names))

feature_names = np.array(feature_names)
print("\nNumber of features after preprocessing:", X_train_trans.shape[1])
print("Feature names length:", len(feature_names))

# if lengths mismatch, create generic names
if X_train_trans.shape[1] != len(feature_names):
    feature_names = np.array([f"f{i}" for i in range(X_train_trans.shape[1])])
    print("Warning: could not recover exact transformed feature names. Using generic names.")

# -------------------------
# Fit models (Linear, RidgeCV, LassoCV)
# -------------------------
# RidgeCV: choose best alpha from grid
alphas = np.logspace(-3, 3, 13)
ridge = RidgeCV(alphas=alphas, scoring="neg_mean_squared_error", cv=5)
lasso = LassoCV(cv=5, n_alphas=100, max_iter=10000, random_state=42)
lin = LinearRegression()

models = {"Linear": lin, "RidgeCV": ridge, "LassoCV": lasso}

fitted = {}
metrics = []
for name, model in models.items():
    model.fit(X_train_trans, y_train)
    y_pred = model.predict(X_test_trans)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    metrics.append({"Model": name, "MSE": mse, "RMSE": rmse, "R2": r2})
    fitted[name] = model
    print(f"\n{name} fitted. MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}")
    # if RidgeCV or LassoCV, report chosen alpha
    if hasattr(model, "alpha_"):
        print(f"  chosen alpha: {model.alpha_}")

metrics_df = pd.DataFrame(metrics).set_index("Model")

# -------------------------
# Coefficients table (align coefficients to feature names)
# -------------------------
coef_data = {"Feature": feature_names}
for name, model in fitted.items():
    coef = getattr(model, "coef_", None)
    if coef is None:
        coef = np.zeros(X_train_trans.shape[1])
    # Some models return shape (n_features,), ensure matching
    coef = np.array(coef).reshape(-1)
    # if mismatch in length, trim or pad
    if len(coef) != len(feature_names):
        if len(coef) > len(feature_names):
            coef = coef[: len(feature_names)]
        else:
            coef = np.concatenate([coef, np.zeros(len(feature_names) - len(coef))])
    coef_data[name] = coef

coef_df = pd.DataFrame(coef_data)
# sort by absolute linear coef for readability
if "Linear" in coef_df.columns:
    coef_df["abs_lin"] = coef_df["Linear"].abs()
    coef_df = coef_df.sort_values("abs_lin", ascending=False).drop(columns=["abs_lin"])
else:
    coef_df = coef_df

# Display results nicely
if have_display_tool:
    display_dataframe_to_user("Model Metrics", metrics_df.reset_index())
    display_dataframe_to_user("Coefficients (post-preprocessing)", coef_df)
else:
    print("\nModel metrics:\n", metrics_df)
    print("\nTop coefficients:\n", coef_df.head(20))

# -------------------------
# Plots: Coefficient comparison & MSE comparison
# -------------------------
plt.figure(figsize=(12, 5))
# show only top 20 features for readability
top_n = min(20, coef_df.shape[0])
coef_plot = coef_df.head(top_n).set_index("Feature")
coef_plot.plot(kind="bar", figsize=(12, 5))
plt.title("Top features: Coefficient comparison (Linear vs Ridge vs Lasso)")
plt.ylabel("Coefficient value (post-preprocessing)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 4))
sns.barplot(x=metrics_df.index, y="MSE", data=metrics_df.reset_index())
plt.title("MSE: Linear vs RidgeCV vs LassoCV")
plt.ylabel("Mean Squared Error")
plt.show()

# -------------------------
# Optional: MSE vs alpha sweep for Ridge and Lasso
# -------------------------
ridge_mses = []
lasso_mses = []
alphas_sweep = np.logspace(-3, 3, 10)
for a in alphas_sweep:
    r = RidgeCV(alphas=[a], scoring="neg_mean_squared_error", cv=5)
    r.fit(X_train_trans, y_train)
    ridge_mses.append(mean_squared_error(y_test, r.predict(X_test_trans)))
    # Lasso with fixed alpha (LassoCV selects alpha internally; to test a given alpha we use Lasso with that alpha)
    from sklearn.linear_model import Lasso
    l = Lasso(alpha=a, max_iter=10000)
    l.fit(X_train_trans, y_train)
    lasso_mses.append(mean_squared_error(y_test, l.predict(X_test_trans)))

plt.figure(figsize=(8, 5))
plt.plot(alphas_sweep, ridge_mses, marker='o', label='Ridge')
plt.plot(alphas_sweep, lasso_mses, marker='o', label='Lasso (fixed alpha)')
plt.xscale('log')
plt.xlabel('Alpha (log scale)')
plt.ylabel('MSE')
plt.title('Regularization: MSE vs Alpha')
plt.legend()
plt.grid(True)
plt.show()

print("\nDone. The pipeline applied imputing, scaling (numeric), and one-hot encoding (categorical).")
print("If you want VIF-based feature removal, polynomial features, or alternate imputation (KNN), tell me and I will add it.")
